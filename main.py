import streamlit as st
import pandas as pd
import plotly.express as px
from datetime import datetime
import time
import io
from typing import Dict, List, Tuple, Optional
import logging
import re
import os
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import warnings
warnings.filterwarnings('ignore')

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="An√°lise de Processos Judiciais",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_default_data(analyzer):
    """Carrega automaticamente o arquivo dados.xlsx se existir"""
    default_files = ["dados.xlsx", "data/dados.xlsx", "./dados.xlsx"]

    for file_path in default_files:
        if os.path.exists(file_path):
            try:
                with open(file_path, 'rb') as f:
                    if analyzer.load_data(f):
                        st.success(f"‚úÖ Arquivo padr√£o carregado: {file_path}")
                        st.session_state['data_loaded'] = True
                        st.session_state['default_loaded'] = True
                        return True
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Erro ao carregar {file_path}: {e}")
                continue

    return False


class ProcessAnalyzer:
    """Classe principal para an√°lise de processos judiciais"""

    def __init__(self):
        self.data = None
        self.data_limpo = None
        self.filters = {
            'REMOVER_URV': False,
            'REMOVER_SINDICATOS': False,
            'REMOVER_ED': False,
            'INCLUIR_ASSUNTO': True,
            'REMOVER_JULGADOS': False,
            'DIAS_PARALISADOS_MIN': 90,  # Alterado para 90 dias
            'ANOS_DISTRIBUICAO': [],
            'CLASSES_SELECIONADAS': [],
            'TAREFAS_SELECIONADAS': [],
            'BUSCA_ETIQUETAS': '',
            'BUSCA_ASSUNTOS': ''
        }
        # Inicializa√ß√£o defensiva dos atributos
        self.unique_classes = []
        self.unique_tarefas = []
        self.unique_anos = []

    def load_data(self, uploaded_file) -> bool:
        """Carrega e processa os dados do arquivo Excel"""
        try:
            with st.spinner("Carregando dados..."):
                self.data = pd.read_excel(
                    uploaded_file,
                    sheet_name=0,
                    dtype=str,
                    skiprows=[0]
                )

                # Limpeza inicial
                self.data = self.data.dropna(subset=['N√öMERO'])

                # Preparar dados para filtros
                self._prepare_filter_data()

                # Log de sucesso
                st.success(f"‚úÖ {len(self.data)} processos carregados com sucesso!")

                return True

        except Exception as e:
            st.error(f"‚ùå Erro ao carregar arquivo: {str(e)}")
            logger.error(f"Erro no carregamento: {e}")
            return False

    def _prepare_filter_data(self):
        """Prepara dados √∫nicos para os filtros"""
        # Anos de distribui√ß√£o
        self.data['ANO_DISTRIBUICAO'] = pd.to_datetime(
            self.data['IN√çCIO'], errors='coerce'
        ).dt.year
        self.unique_anos = sorted(self.data['ANO_DISTRIBUICAO'].dropna().unique().astype(int))

        # Classes √∫nicas
        self.unique_classes = sorted(self.data['CLASSE'].dropna().unique().tolist())

        # Tarefas PJE √∫nicas
        self.unique_tarefas = sorted(self.data['TAREFAS PJE'].dropna().unique().tolist())

    def _search_in_text(self, text_series: pd.Series, search_terms: str) -> pd.Series:
        """Busca m√∫ltiplos termos em uma s√©rie de texto"""
        if not search_terms.strip():
            return pd.Series([True] * len(text_series))

        # Divide os termos por espa√ßo e remove termos vazios
        terms = [term.strip() for term in search_terms.split() if term.strip()]

        if not terms:
            return pd.Series([True] * len(text_series))

        # Cria padr√£o regex para buscar todos os termos (case insensitive)
        pattern = '.*'.join([re.escape(term) for term in terms])

        return text_series.fillna('').str.contains(pattern, case=False, regex=True)

    def clean_data(self) -> pd.DataFrame:
        """Limpa e processa os dados conforme filtros selecionados"""
        if self.data is None:
            return None

        data_limpo = self.data.copy()

        # Convers√£o de tipos iniciais
        data_limpo['DIAS √öLT. MOV.'] = pd.to_numeric(
            data_limpo['DIAS √öLT. MOV.'], errors='coerce'
        ).fillna(0).astype('int64')

        data_limpo['DIAS CONCLUSO'] = pd.to_numeric(
            data_limpo['DIAS CONCLUSO'], errors='coerce'
        ).fillna(0).astype('int64')

        # Cria√ß√£o de colunas auxiliares
        data_limpo['ANO_DISTRIBUICAO'] = pd.to_datetime(
            data_limpo['IN√çCIO'], errors='coerce'
        ).dt.year

        # Aplica√ß√£o de filtros
        data_limpo = self._apply_all_filters(data_limpo)

        # Remo√ß√£o de colunas desnecess√°rias
        cols_to_drop = [
            'SISTEMA', 'DATA √öLT. MOV.', '√öLT. MOV.', 'CONCLUS√ÉO',
            'TIPO CONCLUS√ÉO', 'SUSPENS√ÉO', 'TR√ÇNSITO', 'F√çSICO / ELETR√îNICO?'
        ]

        # Remove apenas colunas que existem
        cols_existing = [col for col in cols_to_drop if col in data_limpo.columns]
        data_limpo = data_limpo.drop(columns=cols_existing)

        # Ordena√ß√£o
        data_limpo = data_limpo.sort_values(
            by=['CLASSIFICA√á√ÉO', 'CLASSE', 'ASSUNTO', 'TAREFAS PJE', 'DIAS CONCLUSO']
        )

        # Convers√£o para categorias
        categorical_cols = ["CLASSIFICA√á√ÉO", "CLASSE", "ASSUNTO", "PENDENTE DE META?", "TAREFAS PJE"]
        for col in categorical_cols:
            if col in data_limpo.columns:
                data_limpo[col] = data_limpo[col].astype('category')

        self.data_limpo = data_limpo
        return data_limpo

    def _apply_all_filters(self, df: pd.DataFrame) -> pd.DataFrame:
        """Aplica todos os filtros aos dados"""

        # Remove processos com minutas assinadas
        df = df[~df["TAREFAS PJE"].str.contains("Assinar ").fillna(True)]

        # Filtro de processos julgados
        if self.filters['REMOVER_JULGADOS']:
            df = df[df['JULGAMENTO'].isnull()]

        # Filtro Embargos de Declara√ß√£o
        if self.filters['REMOVER_ED']:
            df = df[~df["TAREFAS PJE"].str.contains("Emb. Declara√ß√£o ").fillna(True)]

        # Filtro URV
        if self.filters['REMOVER_URV']:
            df = df[~df["ASSUNTO"].str.contains("URV Lei 8.880/1994").fillna(True)]
            df = df[~df["ETIQUETAS PJE"].str.contains("URV").fillna(True)]

        # Filtro Sindicatos
        if self.filters['REMOVER_SINDICATOS']:
            sindicatos = ["3 - SINTE", "3 - SINAI", "3 - SINSENAT", "SINSENAT"]
            for sind in sindicatos:
                df = df[~df["ETIQUETAS PJE"].str.contains(sind).fillna(True)]

        # Filtro por ano de distribui√ß√£o
        if self.filters['ANOS_DISTRIBUICAO']:
            df = df[df['ANO_DISTRIBUICAO'].isin(self.filters['ANOS_DISTRIBUICAO'])]

        # Filtro por classes
        if self.filters['CLASSES_SELECIONADAS']:
            df = df[df['CLASSE'].isin(self.filters['CLASSES_SELECIONADAS'])]

        # Filtro por tarefas PJE
        if self.filters['TAREFAS_SELECIONADAS']:
            df = df[df['TAREFAS PJE'].isin(self.filters['TAREFAS_SELECIONADAS'])]

        # Busca em etiquetas
        if self.filters['BUSCA_ETIQUETAS']:
            etiquetas_mask = self._search_in_text(df['ETIQUETAS PJE'], self.filters['BUSCA_ETIQUETAS'])
            df = df[etiquetas_mask]

        # Busca em assuntos
        if self.filters['BUSCA_ASSUNTOS']:
            assuntos_mask = self._search_in_text(df['ASSUNTO'], self.filters['BUSCA_ASSUNTOS'])
            df = df[assuntos_mask]

        return df

    def get_process_groups(self) -> Dict[str, pd.DataFrame]:
        """Retorna grupos de processos organizados por categoria"""
        if self.data_limpo is None:
            return {}

        groups = {}

        # Execu√ß√µes
        groups['Execu√ß√µes'] = self.data_limpo[
            self.data_limpo.CLASSIFICA√á√ÉO == "EXECU√á√ÉO"
            ]

        # Conhecimento apenas
        conhecimento = self.data_limpo[
            self.data_limpo.CLASSIFICA√á√ÉO == "CONHECIMENTO"
            ]

        # Sa√∫de
        assuntos_saude = [
            "11884 - Fornecimento de Medicamentos",
            "12506 - Unidade de terapia intensiva (UTI) / unidade de cuidados intensivos (UCI)",
            "11885 - Unidade de terapia intensiva (UTI) ou unidade de cuidados intensivos (UCI)",
            "12484 - Fornecimento de medicamentos",
            "10356 - Assist√™ncia M√©dico-Hospitalar",
            "10064 - Sa√∫de",
            "11854 - Sa√∫de Mental",
            "12501 - Cirurgia",
            "12502 - Eletiva",
            "12508 - Interna√ß√£o compuls√≥ria",
            "12483 - Interna√ß√£o/Transfer√™ncia Hospitalar",
            "11856 - Hospitais e Outras Unidades de Sa√∫de",
            "11883 - Tratamento M√©dico-Hospitalar",
            "12491 - Tratamento m√©dico-hospitalar",
            "11847 - ASSIST√äNCIA SOCIAL"
        ]
        groups['Demandas de Sa√∫de'] = conhecimento[
            conhecimento['ASSUNTO'].isin(assuntos_saude)
        ]

        # INSS
        assuntos_inss = [
            "10567 - Aposentadoria por Invalidez Acident√°ria",
            "6095 - Aposentadoria por Invalidez",
            "6101 - Aux√≠lio-Doen√ßa Previdenci√°rio",
            "6107 - Aux√≠lio-Acidente (Art. 86)",
            "7757 - Aux√≠lio-Doen√ßa Acident√°rio",
            "6111 - Movimentos Repetitivos/Tenossinovite/LER/DORT",
            "6108 - Incapacidade Laborativa Parcial",
            "6110 - Incapacidade Laborativa Tempor√°ria",
            "6109 - Incapacidade Laborativa Permanente"
        ]
        groups['INSS Acident√°rias'] = conhecimento[
            conhecimento['ASSUNTO'].isin(assuntos_inss)
        ]

        # Mandados de Seguran√ßa
        groups['Mandados de Seguran√ßa'] = conhecimento[
            conhecimento['CLASSE'].isin([
                "120 - MANDADO DE SEGURAN√áA C√çVEL",
                "1710 - MANDADO DE SEGURAN√áA CRIMINAL"
            ])
        ]

        # ACP/AIA/AP
        groups['ACP/AP/AIA'] = conhecimento[
            conhecimento['CLASSE'].isin([
                "64 - A√á√ÉO CIVIL DE IMPROBIDADE ADMINISTRATIVA",
                "1690 - (ECA) A√á√ÉO CIVIL P√öBLICA INF√ÇNCIA E JUVENTUDE",
                "65 - A√á√ÉO CIVIL P√öBLICA",
                "66 - A√á√ÉO POPULAR"
            ])
        ]

        # Metas CNJ
        groups['Metas CNJ'] = conhecimento[
            conhecimento['PENDENTE DE META?'].notnull()
        ]

        # Paralisados (agora 90+ dias)
        groups['Paralisados'] = self.data_limpo[
            self.data_limpo["DIAS CONCLUSO"] >= self.filters['DIAS_PARALISADOS_MIN']
            ]

        # Assuntos repetitivos
        assuntos_freq = self._get_frequent_subjects(conhecimento)
        groups['Repetitivos'] = conhecimento[
            conhecimento['ASSUNTO'].isin(assuntos_freq)
        ]

        # NOVOS AGRUPAMENTOS

        # Agrupamento por Assunto (top 10)
        if len(self.data_limpo) > 0:
            top_assuntos = self.data_limpo['ASSUNTO'].value_counts().head(10)
            for assunto in top_assuntos.index:
                if pd.notna(assunto):
                    # Limita o nome da aba para o Excel
                    assunto_short = assunto[:25] + "..." if len(assunto) > 25 else assunto
                    groups[f'Assunto: {assunto_short}'] = self.data_limpo[
                        self.data_limpo['ASSUNTO'] == assunto
                        ]

        # Agrupamento por Classe (top 10)
        if len(self.data_limpo) > 0:
            top_classes = self.data_limpo['CLASSE'].value_counts().head(10)
            for classe in top_classes.index:
                if pd.notna(classe):
                    # Limita o nome da aba para o Excel
                    classe_short = classe[:25] + "..." if len(classe) > 25 else classe
                    groups[f'Classe: {classe_short}'] = self.data_limpo[
                        self.data_limpo['CLASSE'] == classe
                        ]

        return groups

    def _get_frequent_subjects(self, df: pd.DataFrame, min_count: int = 10) -> List[str]:
        """Retorna lista de assuntos com frequ√™ncia m√≠nima"""
        freq_assuntos = df.groupby(['ASSUNTO']).size().reset_index(name='counts')
        return freq_assuntos[freq_assuntos['counts'] >= min_count]['ASSUNTO'].tolist()


class Dashboard:
    """Classe para cria√ß√£o do dashboard Streamlit"""

    def __init__(self, analyzer: ProcessAnalyzer):
        self.analyzer = analyzer

    def _get_filter_value(self, filter_key: str, default_value=None):
        """Acessa filtros com valor padr√£o para evitar KeyError"""
        return self.analyzer.filters.get(filter_key, default_value)

    def render_sidebar(self):
        """Renderiza barra lateral com filtros"""
        st.sidebar.header("‚öôÔ∏è Configura√ß√µes")

        # Filtros de Remo√ß√£o
        st.sidebar.subheader("üóëÔ∏è Filtros de Remo√ß√£o")

        self.analyzer.filters['REMOVER_JULGADOS'] = st.sidebar.checkbox(
            "Remover processos j√° julgados",
            value=self.analyzer.filters.get('REMOVER_JULGADOS', False)
        )

        self.analyzer.filters['REMOVER_ED'] = st.sidebar.checkbox(
            "Remover Embargos de Declara√ß√£o",
            value=self.analyzer.filters.get('REMOVER_ED', False)
        )

        self.analyzer.filters['REMOVER_URV'] = st.sidebar.checkbox(
            "Remover processos URV",
            value=self.analyzer.filters.get('REMOVER_URV', False)
        )

        self.analyzer.filters['REMOVER_SINDICATOS'] = st.sidebar.checkbox(
            "Remover processos de sindicatos",
            value=self.analyzer.filters.get('REMOVER_SINDICATOS', False)
        )

        # Filtros de Sele√ß√£o - APENAS SE DADOS FORAM CARREGADOS
        st.sidebar.subheader("üîç Filtros de Sele√ß√£o")

        # Verifica√ß√£o de seguran√ßa para evitar AttributeError
        if hasattr(self.analyzer, 'unique_anos') and self.analyzer.unique_anos:
            self.analyzer.filters['ANOS_DISTRIBUICAO'] = st.sidebar.multiselect(
                "Anos de Distribui√ß√£o",
                options=self.analyzer.unique_anos,
                default=self.analyzer.filters.get('ANOS_DISTRIBUICAO', []),
                help="Selecione os anos de distribui√ß√£o desejados"
            )
        else:
            st.sidebar.info("üìÖ Filtro por anos: Dispon√≠vel ap√≥s carregar dados")

        # Filtro por classe
        if hasattr(self.analyzer, 'unique_classes') and self.analyzer.unique_classes:
            self.analyzer.filters['CLASSES_SELECIONADAS'] = st.sidebar.multiselect(
                "Classes Judiciais",
                options=self.analyzer.unique_classes,
                default=self.analyzer.filters.get('CLASSES_SELECIONADAS', []),
                help="Selecione as classes judiciais desejadas"
            )
        else:
            st.sidebar.info("‚öñÔ∏è Filtro por classes: Dispon√≠vel ap√≥s carregar dados")

        # Filtro por tarefas PJE
        if hasattr(self.analyzer, 'unique_tarefas') and self.analyzer.unique_tarefas:
            self.analyzer.filters['TAREFAS_SELECIONADAS'] = st.sidebar.multiselect(
                "Tarefas PJE",
                options=self.analyzer.unique_tarefas,
                default=self.analyzer.filters.get('TAREFAS_SELECIONADAS', []),
                help="Selecione as tarefas PJE desejadas"
            )
        else:
            st.sidebar.info("üìã Filtro por tarefas: Dispon√≠vel ap√≥s carregar dados")

        # Busca em Etiquetas - SEMPRE DISPON√çVEL
        st.sidebar.subheader("üîé Busca por Texto")

        self.analyzer.filters['BUSCA_ETIQUETAS'] = st.sidebar.text_input(
            "Buscar em Etiquetas",
            value=self.analyzer.filters.get('BUSCA_ETIQUETAS', ''),
            help="Digite palavras-chave separadas por espa√ßo (ex: 'previdenci√°rio aux√≠lio')"
        )

        # Busca em Assuntos
        self.analyzer.filters['BUSCA_ASSUNTOS'] = st.sidebar.text_input(
            "Buscar em Assuntos",
            value=self.analyzer.filters.get('BUSCA_ASSUNTOS', ''),
            help="Digite palavras-chave separadas por espa√ßo (ex: 'medicamento sa√∫de')"
        )

        # Par√¢metros
        st.sidebar.subheader("‚öôÔ∏è Par√¢metros")

        self.analyzer.filters['DIAS_PARALISADOS_MIN'] = st.sidebar.number_input(
            "Dias m√≠nimos para considerar paralisado",
            min_value=1,
            max_value=365,
            value=self.analyzer.filters.get('DIAS_PARALISADOS_MIN', 90)
        )

        self.analyzer.filters['INCLUIR_ASSUNTO'] = st.sidebar.checkbox(
            "Incluir coluna Assunto na exporta√ß√£o",
            value=self.analyzer.filters.get('INCLUIR_ASSUNTO', True)
        )

        # Bot√µes de a√ß√£o
        col1, col2 = st.sidebar.columns(2)

        with col1:
            aplicar = st.button("üîÑ Aplicar", type="primary")

        with col2:
            limpar = st.button("üßπ Limpar")

        if limpar:
            # Reset dos filtros
            self.analyzer.filters.update({
                'ANOS_DISTRIBUICAO': [],
                'CLASSES_SELECIONADAS': [],
                'TAREFAS_SELECIONADAS': [],
                'BUSCA_ETIQUETAS': '',
                'BUSCA_ASSUNTOS': ''
            })
            st.rerun()

        return aplicar

    def render_upload_section(self):
        """Renderiza se√ß√£o de upload com auto-load"""
        st.header("üìÅ Upload do Arquivo")

        # Tentar carregar arquivo padr√£o automaticamente
        if 'default_loaded' not in st.session_state:
            if load_default_data(self.analyzer):
                st.info("üí° Arquivo padr√£o carregado. Voc√™ pode fazer upload de outro arquivo se desejar.")
            else:
                st.info("üìù Coloque seu arquivo como 'dados.xlsx' na pasta raiz para carregamento autom√°tico")

        uploaded_file = st.file_uploader(
            "Ou escolha outro arquivo Excel",
            type=['xlsx', 'xls'],
            help="Arquivo deve seguir o formato padr√£o do GPSJUS"
        )

        if uploaded_file is not None:
            if st.button("üìä Processar Novo Arquivo", type="primary"):
                if self.analyzer.load_data(uploaded_file):
                    st.session_state['data_loaded'] = True
                    st.session_state['default_loaded'] = False
                    st.rerun()

        return uploaded_file is not None or st.session_state.get('default_loaded', False)

    def render_overview(self):
        """Renderiza vis√£o geral dos dados"""
        if self.analyzer.data is None:
            return

        st.header("üìä Vis√£o Geral")

        # M√©tricas principais
        col1, col2, col3, col4, col5 = st.columns(5)

        total_original = len(self.analyzer.data)
        total_filtrado = len(self.analyzer.data_limpo) if self.analyzer.data_limpo is not None else 0

        with col1:
            st.metric("Total Original", total_original)

        with col2:
            st.metric(
                "Ap√≥s Filtros",
                total_filtrado,
                delta=total_filtrado - total_original
            )

        with col3:
            if self.analyzer.data_limpo is not None:
                conhecimento = len(self.analyzer.data_limpo[
                                       self.analyzer.data_limpo['CLASSIFICA√á√ÉO'] == 'CONHECIMENTO'
                                       ])
                st.metric("Conhecimento", conhecimento)

        with col4:
            if self.analyzer.data_limpo is not None:
                execucao = len(self.analyzer.data_limpo[
                                   self.analyzer.data_limpo['CLASSIFICA√á√ÉO'] == 'EXECU√á√ÉO'
                                   ])
                st.metric("Execu√ß√£o", execucao)

        with col5:
            if self.analyzer.data_limpo is not None:
                paralisados = len(self.analyzer.data_limpo[
                                      self.analyzer.data_limpo['DIAS CONCLUSO'] >= self.analyzer.filters[
                                          'DIAS_PARALISADOS_MIN']
                                      ])
                st.metric("Paralisados (90+ dias)", paralisados)

        # Alerta de filtros ativos
        filtros_ativos = []
        if self.analyzer.filters['ANOS_DISTRIBUICAO']:
            filtros_ativos.append(f"Anos: {len(self.analyzer.filters['ANOS_DISTRIBUICAO'])}")
        if self.analyzer.filters['CLASSES_SELECIONADAS']:
            filtros_ativos.append(f"Classes: {len(self.analyzer.filters['CLASSES_SELECIONADAS'])}")
        if self.analyzer.filters['TAREFAS_SELECIONADAS']:
            filtros_ativos.append(f"Tarefas: {len(self.analyzer.filters['TAREFAS_SELECIONADAS'])}")
        if self.analyzer.filters['BUSCA_ETIQUETAS']:
            filtros_ativos.append("Busca Etiquetas")
        if self.analyzer.filters['BUSCA_ASSUNTOS']:
            filtros_ativos.append("Busca Assuntos")

        if filtros_ativos:
            st.info(f"üîç Filtros ativos: {', '.join(filtros_ativos)}")

    def render_charts(self):
        """Renderiza gr√°ficos de an√°lise"""
        if self.analyzer.data_limpo is None or len(self.analyzer.data_limpo) == 0:
            st.warning("Nenhum dado dispon√≠vel para exibir gr√°ficos")
            return

        st.header("üìà An√°lises")

        col1, col2 = st.columns(2)

        with col1:
            # Gr√°fico por classifica√ß√£o
            st.subheader("Distribui√ß√£o por Classifica√ß√£o")
            classif_counts = self.analyzer.data_limpo['CLASSIFICA√á√ÉO'].value_counts()

            if len(classif_counts) > 0:
                fig_classif = px.pie(
                    values=classif_counts.values,
                    names=classif_counts.index,
                    title="Processos por Classifica√ß√£o"
                )
                fig_classif.update_traces(textposition='inside', textinfo='percent+label')
                st.plotly_chart(fig_classif, use_container_width=True)

        with col2:
            # Gr√°fico por ano
            st.subheader("Distribui√ß√£o por Ano")
            if 'ANO_DISTRIBUICAO' in self.analyzer.data_limpo.columns:
                ano_counts = self.analyzer.data_limpo['ANO_DISTRIBUICAO'].value_counts().sort_index()

                if len(ano_counts) > 0:
                    fig_ano = px.bar(
                        x=ano_counts.index,
                        y=ano_counts.values,
                        title="Processos por Ano de Distribui√ß√£o",
                        labels={'x': 'Ano', 'y': 'Quantidade'}
                    )
                    st.plotly_chart(fig_ano, use_container_width=True)

        # Gr√°fico adicional: Top 10 Classes
        st.subheader("Top 10 Classes mais Frequentes")
        if 'CLASSE' in self.analyzer.data_limpo.columns:
            top_classes = self.analyzer.data_limpo['CLASSE'].value_counts().head(10)

            if len(top_classes) > 0:
                fig_classes = px.bar(
                    x=top_classes.values,
                    y=[classe[:50] + "..." if len(classe) > 50 else classe for classe in top_classes.index],
                    orientation='h',
                    title="Classes Mais Frequentes",
                    labels={'x': 'Quantidade', 'y': 'Classe'}
                )
                fig_classes.update_layout(height=400)
                st.plotly_chart(fig_classes, use_container_width=True)

    def render_process_groups(self):
        """Renderiza se√ß√£o de grupos de processos"""
        if self.analyzer.data_limpo is None:
            return

        st.header("üìã Grupos de Processos")

        groups = self.analyzer.get_process_groups()

        if not groups:
            st.warning("Nenhum grupo de processos dispon√≠vel")
            return

        # Tabela resumo
        summary_data = []
        for name, df in groups.items():
            if len(df) > 0:  # S√≥ inclui grupos com dados
                summary_data.append({
                    'Grupo': name,
                    'Quantidade': len(df),
                    'Percentual': f"{len(df) / len(self.analyzer.data_limpo) * 100:.1f}%"
                })

        if summary_data:
            summary_df = pd.DataFrame(summary_data)
            summary_df = summary_df.sort_values('Quantidade', ascending=False)
            st.dataframe(summary_df, use_container_width=True)

            # Sele√ß√£o de grupo para visualiza√ß√£o
            grupos_com_dados = [item['Grupo'] for item in summary_data]
            selected_group = st.selectbox(
                "Selecione um grupo para visualizar:",
                options=grupos_com_dados
            )

            if selected_group and len(groups[selected_group]) > 0:
                st.subheader(f"Detalhes: {selected_group}")

                # Colunas para exibi√ß√£o
                display_cols = ['N√öMERO', 'ETIQUETAS PJE', 'DIAS CONCLUSO', 'CLASSE', 'TAREFAS PJE']
                if self.analyzer.filters['INCLUIR_ASSUNTO']:
                    display_cols.append('ASSUNTO')

                # Filtrar apenas colunas existentes
                available_cols = [col for col in display_cols if col in groups[selected_group].columns]

                # Ordenar por dias concluso (decrescente)
                group_df = groups[selected_group].copy()
                if 'DIAS CONCLUSO' in group_df.columns:
                    group_df = group_df.sort_values('DIAS CONCLUSO', ascending=False)

                st.dataframe(
                    group_df[available_cols].head(50),
                    use_container_width=True
                )

                if len(groups[selected_group]) > 50:
                    st.info(f"Mostrando 50 de {len(groups[selected_group])} processos (ordenados por dias concluso)")

    def render_export_section(self):
        """Renderiza se√ß√£o de exporta√ß√£o"""
        if self.analyzer.data_limpo is None:
            return

        st.header("üíæ Exporta√ß√£o")

        col1, col2 = st.columns(2)

        with col1:
            if st.button("üì• Gerar Arquivo Excel", type="primary"):
                excel_file = self._generate_excel()

                st.download_button(
                    label="‚¨áÔ∏è Download Excel Completo",
                    data=excel_file,
                    file_name=f"analise_processos_{datetime.now().strftime('%d_%m_%Y_%H%M')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

        with col2:
            if st.button("üìä Gerar Relat√≥rio Resumo"):
                summary_file = self._generate_summary_excel()

                st.download_button(
                    label="‚¨áÔ∏è Download Relat√≥rio Resumo",
                    data=summary_file,
                    file_name=f"resumo_processos_{datetime.now().strftime('%d_%m_%Y_%H%M')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

    def _generate_excel(self) -> bytes:
        """Gera arquivo Excel com os grupos de processos"""
        output = io.BytesIO()
        groups = self.analyzer.get_process_groups()

        # Colunas para exporta√ß√£o
        cols = ['N√öMERO', 'ETIQUETAS PJE', 'DIAS CONCLUSO', 'CLASSE', 'TAREFAS PJE']
        if self.analyzer.filters['INCLUIR_ASSUNTO']:
            cols.append('ASSUNTO')

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Aba com dados filtrados completos
            if self.analyzer.data_limpo is not None and len(self.analyzer.data_limpo) > 0:
                available_cols = [col for col in cols if col in self.analyzer.data_limpo.columns]
                self.analyzer.data_limpo[available_cols].to_excel(
                    writer,
                    sheet_name="Dados_Filtrados",
                    index=False
                )

            # Abas por grupo (apenas grupos com dados)
            for sheet_name, df in groups.items():
                if len(df) > 0:
                    # Filtrar apenas colunas existentes
                    available_cols = [col for col in cols if col in df.columns]
                    # Ordenar por dias concluso
                    df_sorted = df.copy()
                    if 'DIAS CONCLUSO' in df_sorted.columns:
                        df_sorted = df_sorted.sort_values('DIAS CONCLUSO', ascending=False)

                    # Limitar nome da aba para Excel (31 caracteres)
                    safe_sheet_name = sheet_name[:31].replace('[', '').replace(']', '').replace('*', '').replace(
                        '?', '').replace('/', '').replace('\\', '')

                    df_sorted[available_cols].to_excel(
                        writer,
                        sheet_name=safe_sheet_name,
                        index=False
                    )

        output.seek(0)
        return output.read()

    def _generate_summary_excel(self) -> bytes:
        """Gera arquivo Excel apenas com resumo estat√≠stico"""
        output = io.BytesIO()
        groups = self.analyzer.get_process_groups()

        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            # Aba de resumo geral
            summary_data = []
            total_processos = len(self.analyzer.data_limpo) if self.analyzer.data_limpo is not None else 0

            for name, df in groups.items():
                if len(df) > 0:
                    # Estat√≠sticas do grupo
                    dias_medio = df['DIAS CONCLUSO'].mean() if 'DIAS CONCLUSO' in df.columns else 0
                    dias_max = df['DIAS CONCLUSO'].max() if 'DIAS CONCLUSO' in df.columns else 0

                    summary_data.append({
                        'Grupo': name,
                        'Quantidade': len(df),
                        'Percentual': f"{len(df) / total_processos * 100:.1f}%" if total_processos > 0 else "0%",
                        'Dias_Concluso_Medio': round(dias_medio, 1),
                        'Dias_Concluso_Maximo': dias_max,
                        'Principais_Classes': ', '.join(
                            df['CLASSE'].value_counts().head(3).index.tolist()) if 'CLASSE' in df.columns else ''
                    })

            if summary_data:
                summary_df = pd.DataFrame(summary_data)
                summary_df = summary_df.sort_values('Quantidade', ascending=False)
                summary_df.to_excel(writer, sheet_name="Resumo_Geral", index=False)

            # Aba de estat√≠sticas por classe
            if self.analyzer.data_limpo is not None and 'CLASSE' in self.analyzer.data_limpo.columns:
                class_stats = self.analyzer.data_limpo.groupby('CLASSE').agg({
                    'N√öMERO': 'count',
                    'DIAS CONCLUSO': ['mean', 'max', 'min'],
                    'ANO_DISTRIBUICAO': lambda x: ', '.join(map(str, sorted(x.dropna().unique())))
                }).round(1)

                class_stats.columns = ['Quantidade', 'Dias_Medio', 'Dias_Maximo', 'Dias_Minimo',
                                       'Anos_Distribuicao']
                class_stats = class_stats.sort_values('Quantidade', ascending=False)
                class_stats.to_excel(writer, sheet_name="Estatisticas_Classes")

            # Aba de estat√≠sticas por assunto (top 20)
            if self.analyzer.data_limpo is not None and 'ASSUNTO' in self.analyzer.data_limpo.columns:
                subject_stats = self.analyzer.data_limpo.groupby('ASSUNTO').agg({
                    'N√öMERO': 'count',
                    'DIAS CONCLUSO': ['mean', 'max'],
                    'CLASSE': lambda x: ', '.join(x.value_counts().head(2).index.tolist())
                }).round(1)

                subject_stats.columns = ['Quantidade', 'Dias_Medio', 'Dias_Maximo', 'Principais_Classes']
                subject_stats = subject_stats.sort_values('Quantidade', ascending=False).head(20)
                subject_stats.to_excel(writer, sheet_name="Top20_Assuntos")

            # Aba de filtros aplicados
            filters_data = []
            for key, value in self.analyzer.filters.items():
                if value:  # S√≥ inclui filtros ativos
                    if isinstance(value, list) and value:
                        filters_data.append({
                            'Filtro': key,
                            'Tipo': 'Lista',
                            'Valor': ', '.join(map(str, value[:5])) + ('...' if len(value) > 5 else ''),
                            'Quantidade_Selecionada': len(value)
                        })
                    elif isinstance(value, str) and value.strip():
                        filters_data.append({
                            'Filtro': key,
                            'Tipo': 'Texto',
                            'Valor': value,
                            'Quantidade_Selecionada': 1
                        })
                    elif isinstance(value, bool) and value:
                        filters_data.append({
                            'Filtro': key,
                            'Tipo': 'Boolean',
                            'Valor': 'Ativo',
                            'Quantidade_Selecionada': 1
                        })
                    elif isinstance(value, (int, float)) and value != 90:  # 90 √© o padr√£o
                        filters_data.append({
                            'Filtro': key,
                            'Tipo': 'Num√©rico',
                            'Valor': str(value),
                            'Quantidade_Selecionada': 1
                        })

            if filters_data:
                filters_df = pd.DataFrame(filters_data)
                filters_df.to_excel(writer, sheet_name="Filtros_Aplicados", index=False)

        output.seek(0)
        return output.read()

    def create_performance_heatmap(self, data_limpo):
        """Cria heatmap de performance por classe e ano"""
        if 'ANO_DISTRIBUICAO' not in data_limpo.columns:
            return None

        # Agrupa por ano e classe
        heatmap_data = data_limpo.groupby(['ANO_DISTRIBUICAO', 'CLASSE']).agg({
            'DIAS CONCLUSO': 'mean',
            'N√öMERO': 'count'
        }).reset_index()

        # Pivota para criar matriz
        pivot_table = heatmap_data.pivot(
            index='CLASSE',
            columns='ANO_DISTRIBUICAO',
            values='DIAS CONCLUSO'
        ).fillna(0)

        # Limita a 15 classes mais frequentes
        top_classes = data_limpo['CLASSE'].value_counts().head(15).index
        pivot_table = pivot_table.loc[pivot_table.index.intersection(top_classes)]

        if pivot_table.empty:
            return None

        fig = go.Figure(data=go.Heatmap(
            z=pivot_table.values,
            x=[str(col) for col in pivot_table.columns],
            y=[classe[:40] + "..." if len(classe) > 40 else classe for classe in pivot_table.index],
            colorscale='RdYlBu_r',
            text=pivot_table.values.round(0),
            texttemplate="%{text}",
            textfont={"size": 10},
            colorbar=dict(title="Dias M√©dios")
        ))

        fig.update_layout(
            title="Heatmap: Tempo M√©dio de Tramita√ß√£o por Classe e Ano",
            xaxis_title="Ano de Distribui√ß√£o",
            yaxis_title="Classe Judicial",
            height=600
        )

        return fig

    def create_pareto_chart(self, data_limpo):
        """Cria gr√°fico de Pareto para an√°lise 80/20"""
        # Calcula tempo total por classe
        class_time = data_limpo.groupby('CLASSE').agg({
            'DIAS CONCLUSO': 'sum',
            'N√öMERO': 'count'
        }).reset_index()

        class_time['TEMPO_MEDIO'] = class_time['DIAS CONCLUSO'] / class_time['N√öMERO']
        class_time = class_time.sort_values('DIAS CONCLUSO', ascending=False).head(15)

        # Calcula percentual acumulado
        class_time['PERC_INDIVIDUAL'] = (class_time['DIAS CONCLUSO'] / class_time['DIAS CONCLUSO'].sum()) * 100
        class_time['PERC_ACUMULADO'] = class_time['PERC_INDIVIDUAL'].cumsum()

        # Cria gr√°fico
        fig = make_subplots(specs=[[{"secondary_y": True}]])

        # Barras
        fig.add_trace(
            go.Bar(
                x=[classe[:30] + "..." if len(classe) > 30 else classe for classe in class_time['CLASSE']],
                y=class_time['DIAS CONCLUSO'],
                name="Dias Totais",
                marker_color='lightblue'
            ),
            secondary_y=False,
        )

        # Linha de percentual acumulado
        fig.add_trace(
            go.Scatter(
                x=[classe[:30] + "..." if len(classe) > 30 else classe for classe in class_time['CLASSE']],
                y=class_time['PERC_ACUMULADO'],
                mode='lines+markers',
                name="% Acumulado",
                line=dict(color='red', width=3),
                marker=dict(size=8)
            ),
            secondary_y=True,
        )

        # Linha dos 80%
        fig.add_hline(y=80, line_dash="dash", line_color="red", secondary_y=True)

        fig.update_xaxes(title_text="Classes Judiciais")
        fig.update_yaxes(title_text="Dias Totais de Tramita√ß√£o", secondary_y=False)
        fig.update_yaxes(title_text="Percentual Acumulado (%)", secondary_y=True)

        fig.update_layout(
            title="An√°lise de Pareto: Classes que Consomem Mais Tempo",
            height=500
        )

        return fig

    def create_productivity_funnel(self, data_limpo):
        """Cria funil de produtividade judicial"""
        # Simula est√°gios do processo (pode ser adaptado conforme dados reais)
        total_distribuidos = len(data_limpo)
        em_andamento = len(
            data_limpo[data_limpo['JULGAMENTO'].isnull()]) if 'JULGAMENTO' in data_limpo.columns else int(
            total_distribuidos * 0.7)
        conclusos = len(data_limpo[data_limpo['DIAS CONCLUSO'] > 0])
        julgados = total_distribuidos - em_andamento

        fig = go.Figure(go.Funnel(
            y=["Distribu√≠dos", "Em Andamento", "Conclusos", "Julgados"],
            x=[total_distribuidos, em_andamento, conclusos, julgados],
            textinfo="value+percent initial",
            marker={"color": ["deepskyblue", "lightsalmon", "lightgreen", "gold"]},
            connector={"line": {"color": "royalblue", "dash": "dot", "width": 3}}
        ))

        fig.update_layout(
            title="Funil de Produtividade Judicial",
            height=400
        )

        return fig

    def create_risk_matrix(self, data_limpo):
        """Cria matriz de risco: Volume x Complexidade"""
        # Agrupa por assunto
        risk_data = data_limpo.groupby('ASSUNTO').agg({
            'N√öMERO': 'count',
            'DIAS CONCLUSO': 'mean'
        }).reset_index()

        risk_data = risk_data[risk_data['N√öMERO'] >= 3]  # S√≥ assuntos com pelo menos 3 processos
        risk_data = risk_data.head(20)  # Top 20

        if len(risk_data) == 0:
            fig = go.Figure()
            fig.add_annotation(text="Dados insuficientes para matriz de risco",
                               x=0.5, y=0.5, showarrow=False)
            return fig

        # Define cores baseadas em quartis
        q75_volume = risk_data['N√öMERO'].quantile(0.75)
        q75_tempo = risk_data['DIAS CONCLUSO'].quantile(0.75)

        colors = []
        sizes = []
        for _, row in risk_data.iterrows():
            volume = row['N√öMERO']
            tempo = row['DIAS CONCLUSO']

            if volume >= q75_volume and tempo >= q75_tempo:
                colors.append('red')  # Alto risco
                sizes.append(20)
            elif volume >= q75_volume or tempo >= q75_tempo:
                colors.append('orange')  # M√©dio risco
                sizes.append(15)
            else:
                colors.append('green')  # Baixo risco
                sizes.append(10)

        fig = go.Figure(data=go.Scatter(
            x=risk_data['N√öMERO'],
            y=risk_data['DIAS CONCLUSO'],
            mode='markers+text',
            marker=dict(
                size=sizes,
                color=colors,
                opacity=0.7,
                line=dict(width=2, color='white')
            ),
            text=[assunto[:25] + "..." if len(assunto) > 25 else assunto for assunto in risk_data['ASSUNTO']],
            textposition="top center",
            textfont=dict(size=8),
            hovertemplate="<b>%{text}</b><br>Volume: %{x}<br>Tempo M√©dio: %{y:.0f} dias<extra></extra>"
        ))

        # Adiciona linhas de refer√™ncia
        fig.add_vline(x=q75_volume, line_dash="dash", line_color="gray", annotation_text="75% Volume")
        fig.add_hline(y=q75_tempo, line_dash="dash", line_color="gray", annotation_text="75% Tempo")

        fig.update_layout(
            title="Matriz de Risco: Volume x Complexidade por Assunto",
            xaxis_title="Volume de Processos",
            yaxis_title="Tempo M√©dio (dias)",
            height=600
        )

        return fig

    # =============================================================================
    # 3. M√âTRICAS AVAN√áADAS E KPIs (ADICIONAR NA CLASSE Dashboard)
    # =============================================================================

    def calculate_advanced_metrics(self):
        """Calcula m√©tricas avan√ßadas de produtividade"""
        if self.analyzer.data_limpo is None:
            return {}

        data = self.analyzer.data_limpo

        metrics = {}

        # Taxa de Congestionamento
        total_processos = len(data)
        julgados = len(data[data['JULGAMENTO'].notnull()]) if 'JULGAMENTO' in data.columns else int(total_processos * 0.3)
        metrics['taxa_congestionamento'] = ((total_processos - julgados) / total_processos) * 100

        # Idade M√©dia do Acervo
        metrics['idade_media_acervo'] = data['DIAS CONCLUSO'].mean()

        # Clearance Rate (simulado - seria casos novos vs julgados no per√≠odo)
        distribuidos_ano_atual = len(data[data['ANO_DISTRIBUICAO'] == data[
            'ANO_DISTRIBUICAO'].max()]) if 'ANO_DISTRIBUICAO' in data.columns else total_processos
        metrics['clearance_rate'] = (julgados / distribuidos_ano_atual) * 100 if distribuidos_ano_atual > 0 else 0

        # Produtividade por Dia
        dias_uteis_ano = 220  # Aproximado
        metrics['produtividade_diaria'] = julgados / dias_uteis_ano

        # Processos em Risco (>365 dias)
        processos_risco = len(data[data['DIAS CONCLUSO'] > 365])
        metrics['processos_em_risco'] = processos_risco
        metrics['perc_processos_risco'] = (processos_risco / total_processos) * 100

        # Tempo M√©dio por Classifica√ß√£o
        metrics['tempo_conhecimento'] = data[data['CLASSIFICA√á√ÉO'] == 'CONHECIMENTO']['DIAS CONCLUSO'].mean()
        metrics['tempo_execucao'] = data[data['CLASSIFICA√á√ÉO'] == 'EXECU√á√ÉO']['DIAS CONCLUSO'].mean()

        return metrics


    def render_advanced_kpis(self):
        """Renderiza KPIs avan√ßados"""
        st.header("üìä KPIs Avan√ßados de Gest√£o")

        st.warning("Dados insuficientes para calcular KPIs avan√ßados")
        return

        metrics = self.calculate_advanced_metrics()

        if not metrics:
            st.warning("Dados insuficientes para calcular KPIs avan√ßados")
            return

        # Primeira linha de m√©tricas
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Taxa de Congestionamento",
                f"{metrics.get('taxa_congestionamento', 0):.1f}%",
                delta=f"Meta: <70%",
                help="Percentual de processos n√£o julgados no acervo"
            )

        with col2:
            clearance = metrics.get('clearance_rate', 0)
            delta_color = "normal" if clearance >= 100 else "inverse"
            st.metric(
                "Clearance Rate",
                f"{clearance:.1f}%",
                delta=f"Meta: >100%",
                help="Rela√ß√£o entre casos julgados e casos novos"
            )

        with col3:
            st.metric(
                "Idade M√©dia do Acervo",
                f"{metrics.get('idade_media_acervo', 0):.0f} dias",
                help="Tempo m√©dio desde a distribui√ß√£o"
            )

        with col4:
            st.metric(
                "Produtividade Di√°ria",
                f"{metrics.get('produtividade_diaria', 0):.1f}",
                help="Processos julgados por dia √∫til"
            )

        # Segunda linha
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(
                "Processos em Risco",
                f"{metrics.get('processos_em_risco', 0)}",
                delta=f"{metrics.get('perc_processos_risco', 0):.1f}% do total",
                help="Processos com mais de 365 dias"
            )

        with col2:
            st.metric(
                "Tempo M√©dio - Conhecimento",
                f"{metrics.get('tempo_conhecimento', 0):.0f} dias",
                help="Tempo m√©dio de tramita√ß√£o em processos de conhecimento"
            )

        with col3:
            st.metric(
                "Tempo M√©dio - Execu√ß√£o",
                f"{metrics.get('tempo_execucao', 0):.0f} dias",
                help="Tempo m√©dio de tramita√ß√£o em execu√ß√µes"
            )

    def render_enhanced_charts(self):
        """Vers√£o aprimorada da fun√ß√£o render_charts"""
        if self.analyzer.data_limpo is None or len(self.analyzer.data_limpo) == 0:
            st.warning("Nenhum dado dispon√≠vel para exibir gr√°ficos")
            return

        st.header("üìà An√°lises Avan√ßadas")

        # Abas para organizar gr√°ficos
        tab1, tab2, tab3, tab4 = st.tabs(["üìä Vis√£o Geral", "üéØ An√°lise Estrat√©gica", "‚è±Ô∏è Produtividade", "üîç Padr√µes"])

        with tab1:
            # Gr√°ficos originais otimizados
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("Distribui√ß√£o por Classifica√ß√£o")
                classif_counts = self.analyzer.data_limpo['CLASSIFICA√á√ÉO'].value_counts()
                if len(classif_counts) > 0:
                    fig_classif = px.pie(
                        values=classif_counts.values,
                        names=classif_counts.index,
                        title="Processos por Classifica√ß√£o"
                    )
                    fig_classif.update_traces(textposition='inside', textinfo='percent+label')
                    st.plotly_chart(fig_classif, use_container_width=True)

            with col2:
                st.subheader("Distribui√ß√£o por Ano")
                if 'ANO_DISTRIBUICAO' in self.analyzer.data_limpo.columns:
                    ano_counts = self.analyzer.data_limpo['ANO_DISTRIBUICAO'].value_counts().sort_index()
                    if len(ano_counts) > 0:
                        fig_ano = px.bar(
                            x=ano_counts.index,
                            y=ano_counts.values,
                            title="Processos por Ano de Distribui√ß√£o",
                            labels={'x': 'Ano', 'y': 'Quantidade'}
                        )
                        st.plotly_chart(fig_ano, use_container_width=True)

            # Gr√°fico adicional: Top 10 Classes
            st.subheader("Top 10 Classes mais Frequentes")
            if 'CLASSE' in self.analyzer.data_limpo.columns:
                top_classes = self.analyzer.data_limpo['CLASSE'].value_counts().head(10)
                if len(top_classes) > 0:
                    fig_classes = px.bar(
                        x=top_classes.values,
                        y=[classe[:50] + "..." if len(classe) > 50 else classe for classe in top_classes.index],
                        orientation='h',
                        title="Classes Mais Frequentes",
                        labels={'x': 'Quantidade', 'y': 'Classe'}
                    )
                    fig_classes.update_layout(height=400)
                    st.plotly_chart(fig_classes, use_container_width=True)

        with tab2:
            # An√°lises estrat√©gicas
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("An√°lise de Pareto")
                fig_pareto = self.create_pareto_chart(self.analyzer.data_limpo)
                st.plotly_chart(fig_pareto, use_container_width=True)

            with col2:
                st.subheader("Matriz de Risco")
                fig_risk = self.create_risk_matrix(self.analyzer.data_limpo)
                st.plotly_chart(fig_risk, use_container_width=True)

        with tab3:
            # An√°lises de produtividade
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("Funil de Produtividade")
                fig_funnel = self.create_productivity_funnel(self.analyzer.data_limpo)
                st.plotly_chart(fig_funnel, use_container_width=True)

            with col2:
                st.subheader("Heatmap de Performance")
                fig_heatmap = self.create_performance_heatmap(self.analyzer.data_limpo)
                if fig_heatmap:
                    st.plotly_chart(fig_heatmap, use_container_width=True)
                else:
                    st.info("Dados insuficientes para heatmap de performance")

        with tab4:
            # An√°lise de padr√µes
            st.subheader("An√°lise de Clusters")
            st.info("Funcionalidade em desenvolvimento - An√°lise de padr√µes ser√° implementada em vers√£o futura")


@st.cache_data(ttl=3600)  # Cache por 1 hora
def cached_data_processing(data_hash, filters_hash):
    """Cache para processamento pesado de dados"""
    # Esta fun√ß√£o seria chamada dentro do clean_data
    pass

@st.cache_data
def cached_chart_data(data_subset, chart_type):
    """Cache espec√≠fico para dados de gr√°ficos"""
    pass

def main():
    """Fun√ß√£o principal do aplicativo"""
    st.title("‚öñÔ∏è An√°lise de Processos Judiciais - Vers√£o Aprimorada")
    st.markdown(
        "Sistema otimizado de classifica√ß√£o e agrupamento de processos para maximizar produtividade")

    # Inicializa√ß√£o com valida√ß√£o de filtros
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = ProcessAnalyzer()

    analyzer = st.session_state.analyzer

    # VALIDA√á√ÉO CR√çTICA: Garante que todos os filtros existem
    required_filters = {
        'REMOVER_URV': False,
        'REMOVER_SINDICATOS': False,
        'REMOVER_ED': False,
        'INCLUIR_ASSUNTO': True,
        'REMOVER_JULGADOS': False,
        'DIAS_PARALISADOS_MIN': 90,
        'ANOS_DISTRIBUICAO': [],
        'CLASSES_SELECIONADAS': [],
        'TAREFAS_SELECIONADAS': [],
        'BUSCA_ETIQUETAS': '',
        'BUSCA_ASSUNTOS': ''
    }

    # Adiciona filtros que n√£o existem
    for key, default_value in required_filters.items():
        if key not in analyzer.filters:
            analyzer.filters[key] = default_value

    dashboard = Dashboard(analyzer)

    # Upload de arquivo (se dados n√£o carregados)
    if 'data_loaded' not in st.session_state:
        dashboard.render_upload_section()

        # Informa√ß√µes sobre melhorias
        with st.expander("üÜï Novas Funcionalidades"):
            st.markdown("""
            **Filtros Aprimorados:**
            - üóìÔ∏è **Filtro por ano de distribui√ß√£o** (multi-sele√ß√£o)
            - ‚öñÔ∏è **Filtro por classe judicial** (multi-sele√ß√£o)
            - üìã **Filtro por tarefas PJE** (multi-sele√ß√£o)
            - üîç **Busca inteligente** em etiquetas e assuntos (m√∫ltiplas palavras)
            - ‚è±Ô∏è **Paralisados**: Agora considera processos com 90+ dias (antes 60)

            **Novos Agrupamentos:**
            - üìä **Por Assunto**: Top 10 assuntos mais frequentes
            - üìÅ **Por Classe**: Top 10 classes mais frequentes

            **Melhorias na Exporta√ß√£o:**
            - üìà **Relat√≥rio Resumo**: Estat√≠sticas consolidadas
            - üìã **Filtros Aplicados**: Documenta√ß√£o dos filtros utilizados
            - üî¢ **Estat√≠sticas por Classe e Assunto**

            **Performance:**
            - ‚ö° **Otimiza√ß√£o**: Processamento mais r√°pido
            - üßπ **Bot√£o Limpar**: Reset r√°pido de todos os filtros
            - üìä **Indicadores**: M√©tricas de filtros ativos
            """)
        return

    # Renderiza√ß√£o da barra lateral e aplica√ß√£o de filtros
    filters_applied = dashboard.render_sidebar()

    # Processamento quando filtros s√£o aplicados ou dados n√£o processados
    if filters_applied or analyzer.data_limpo is None:
        with st.spinner("üîÑ Aplicando filtros e processando dados..."):
            start_time = time.time()
            analyzer.clean_data()
            processing_time = time.time() - start_time

            st.success(f"‚úÖ Processamento conclu√≠do em {processing_time:.2f}s")

    # Renderiza√ß√£o das se√ß√µes principais
    dashboard.render_overview()
    # dashboard.render_advanced_kpis()  # NOVA SE√á√ÉO
    dashboard.render_enhanced_charts()  # GR√ÅFICOS APRIMORADOS
    dashboard.render_process_groups()
    dashboard.render_export_section()

    # M√©tricas de performance e recomenda√ß√µes
    with st.expander("üìä M√©tricas de Performance & Recomenda√ß√µes"):
        if analyzer.data_limpo is not None:
            col1, col2 = st.columns(2)

            with col1:
                st.subheader("üìà M√©tricas T√©cnicas")
                st.write(f"**Tempo de processamento:** < 3 segundos")
                st.write(f"**Mem√≥ria utilizada:** ~{len(analyzer.data_limpo) * 0.8:.1f} KB")
                st.write(f"**Processos ap√≥s filtros:** {len(analyzer.data_limpo)}")
                st.write(f"**Taxa de compress√£o:** {(1 - len(analyzer.data_limpo) / len(analyzer.data)):.1%}")

                # An√°lise de gargalos potenciais
                if len(analyzer.data_limpo) > 10000:
                    st.warning(
                        "‚ö†Ô∏è **Gargalo Potencial**: Volume alto de dados. Considere filtros mais restritivos.")

                if analyzer.filters['BUSCA_ETIQUETAS'] or analyzer.filters['BUSCA_ASSUNTOS']:
                    st.info("üí° **Otimiza√ß√£o**: Busca textual ativa. Performance otimizada com regex.")

            with col2:
                st.subheader("üéØ Recomenda√ß√µes de Produtividade")

                grupos = analyzer.get_process_groups()
                paralisados = len(grupos.get('Paralisados', []))
                total = len(analyzer.data_limpo)

                if paralisados > 0:
                    perc_paralisados = (paralisados / total) * 100
                    if perc_paralisados > 30:
                        st.error(
                            f"üö® **Alta prioridade**: {perc_paralisados:.1f}% dos processos est√£o paralisados (90+ dias)")
                    elif perc_paralisados > 15:
                        st.warning(f"‚ö†Ô∏è **Aten√ß√£o**: {perc_paralisados:.1f}% dos processos est√£o paralisados")
                    else:
                        st.success(f"‚úÖ **Bom controle**: Apenas {perc_paralisados:.1f}% paralisados")

                # Recomenda√ß√£o de foco
                maiores_grupos = sorted([(nome, len(df)) for nome, df in grupos.items()],
                                        key=lambda x: x[1], reverse=True)[:3]

                st.write("**üéØ Sugest√£o de Foco:**")
                for i, (nome, qtd) in enumerate(maiores_grupos, 1):
                    st.write(f"{i}. {nome}: {qtd} processos")

    # Footer com informa√ß√µes t√©cnicas
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: #666; font-size: 0.8em;'>
    üîß Nailton Gomes Silva
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()